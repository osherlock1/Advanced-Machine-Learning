{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Optuna Workshop**\n",
        "\n",
        "\n",
        "In this workshop you will learn about **Optuna**, an open source hyper parameter optimization framework to automate hyperparameter search.\n",
        "\n",
        "\n",
        "**You will learn**\n",
        "\n",
        "  -Basic Functions of Optuna\n",
        "\n",
        "  -How set up a model to be optimized\n",
        "\n",
        "  -How to run a study\n",
        "  \n",
        "  -How to visualzie resunts with Optuna"
      ],
      "metadata": {
        "id": "MNQi_XCE6wyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step1: Install and Import Required Libraries**\n",
        "\n",
        "To use optuna all you need to do is\n",
        "\n",
        "**download:** !pip install optuna\n",
        "\n",
        "**import:** import optuna"
      ],
      "metadata": {
        "id": "uVoommUn7I96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N1ASVi6k7In6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from optuna.importance import get_param_importances\n",
        "from optuna.visualization import plot_param_importances\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
        "import torchvision\n",
        "\n",
        "\n",
        "#CIFAR-10 Dataset libraries\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#Set Random Seed\n",
        "torch.manual_seed(41)\n",
        "np.random.seed(41)"
      ],
      "metadata": {
        "id": "cYgjO88U7l9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step2: Import Dataset (CIFAR)**\n",
        "\n",
        "For easy testing we will use the CIFAR dataset"
      ],
      "metadata": {
        "id": "86hVvkkj8RXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data augmentation transform\n",
        "\n",
        "aug_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)) # standard CIFAR values\n",
        "])\n",
        "\n",
        "#Load in Dataset into train and test sets\n",
        "\n",
        "cifar_trainset = datasets.CIFAR10(root='./data', train=True, download = True, transform = aug_transform)\n",
        "\n",
        "cifar_testset = datasets.CIFAR10(root='./data', train=False, download = True, transform = aug_transform)"
      ],
      "metadata": {
        "id": "wYtxHBnd8V4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split up the Train Set**"
      ],
      "metadata": {
        "id": "35tiC88P9W5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_dataset = cifar_trainset\n",
        "test_dataset = cifar_testset\n",
        "\n",
        "#Get Split lengths\n",
        "data_size = len(train_dataset)\n",
        "train_size = int(0.8 * data_size)\n",
        "val_size = int(data_size - train_size)\n",
        "\n",
        "#Split up dataset\n",
        "train_set, val_set = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n"
      ],
      "metadata": {
        "id": "9jjes2fo9aAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create DataLoaders**"
      ],
      "metadata": {
        "id": "wUa1ukKQ9eK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Determine Batch Sizez\n",
        "batch_size = 64\n",
        "\n",
        "#Define Loader (Will leave drop last on = true for simplicity of chaning batch size with no size errors during testing)\n",
        "train_loader = DataLoader(train_set, batch_size, shuffle = True, drop_last = True)\n",
        "val_loader = DataLoader(val_set, batch_size, shuffle = True, drop_last = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle = False, drop_last =True)"
      ],
      "metadata": {
        "id": "kDGgTW5-9fdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step3: Define the CNN Model**\n",
        "\n",
        "Simple CNN model for image classification\n",
        "\n",
        "-We will define the model class with a dropout rate paramter for optimization"
      ],
      "metadata": {
        "id": "agjZk-E094ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCNN(nn.Module):\n",
        "  def __init__(self, dropout_rate=0.5):\n",
        "    super(ImageCNN, self).__init__()\n",
        "\n",
        "    #Define Convolutional Layers\n",
        "    self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 16, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 16, out_channels = 16, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n",
        "\n",
        "    #Define Pooling Layer\n",
        "    self.pool = nn.MaxPool2d(kernel_size = 2, stride= 2)\n",
        "\n",
        "    #Define Fully Connected Layers\n",
        "    self.fc1 = nn.Linear(4 * 4 * 32, 10)\n",
        "\n",
        "    #Define Dropout\n",
        "    self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "E9iJcKD197Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step4: Training the Model**\n",
        "\n",
        "Do an intial round of training as a benchmark"
      ],
      "metadata": {
        "id": "z8hM8vKZPjGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize the Model**"
      ],
      "metadata": {
        "id": "7TO0Pwz7P8wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = ImageCNN().to(device)\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Model Parameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "\n",
        "epochs = 10\n",
        "N_TRAIN_EXAMPLES = batch_size * 30 #Specify the number of batches we will use in one epoch for quicker training\n",
        "N_VAL_EXAMPLES = batch_size * 10"
      ],
      "metadata": {
        "id": "dzAoFBmGPmIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform Initial Training**"
      ],
      "metadata": {
        "id": "cWT-_uTvQeeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#For visualization later\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "\n",
        "#Define function for grid search\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Start MLflow run\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "      model.train()\n",
        "      data, labels = data.to(device), labels.to(device)\n",
        "      if batch_idx * batch_size >= N_TRAIN_EXAMPLES: #Reduce the number of batches for each epoch\n",
        "        break\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #Forward pass\n",
        "      y_pred = model(data)\n",
        "      loss = criterion(y_pred, labels)\n",
        "\n",
        "      #Backward pass\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Accumulate the loss\n",
        "      running_loss += loss.item() * data.size(0)\n",
        "\n",
        "      # Calculate accuracy for this batch and accumulate correct predictions\n",
        "      _, predicted = torch.max(y_pred, 1)\n",
        "      running_corrects += (predicted == labels).sum().item()\n",
        "      total_samples += labels.size(0)\n",
        "\n",
        "\n",
        "    #--Validation--\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    val_running_loss = 0.0\n",
        "    val_running_corrects = 0\n",
        "    val_total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, labels) in enumerate(val_loader):\n",
        "\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        if batch_idx * batch_size >= N_TRAIN_EXAMPLES: #Reduce the number of batches for each epoch\n",
        "          break\n",
        "\n",
        "        y_val = model(data)\n",
        "        val_loss = criterion(y_val, labels)\n",
        "\n",
        "        #Accumulate Validation Loss\n",
        "        val_running_loss += val_loss.item() * data.size(0)\n",
        "\n",
        "\n",
        "        #Calculate validation accuracy\n",
        "        _, val_predicted = torch.max(y_val,1)\n",
        "        val_running_corrects += (val_predicted == labels).sum().item()\n",
        "\n",
        "        val_total_samples += labels.size(0)\n",
        "\n",
        "\n",
        "\n",
        "    # Calculate the average loss and overall accuracy for this epoch\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_accuracy = running_corrects / total_samples\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.4f}\")\n",
        "\n",
        "    #Calc average validation loss and accuracy for epoch\n",
        "    epoch_val_loss = val_running_loss / val_total_samples\n",
        "    epoch_val_accuracy = val_running_corrects / val_total_samples\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_accuracy:.4f}')\n",
        "\n",
        "\n",
        "    ###Train and Validation loss for plotting\n",
        "    train_losses.append(epoch_loss)\n",
        "    val_losses.append(epoch_val_loss)\n",
        "\n",
        "    ###Train and validation accuracies for plotting\n",
        "    train_accuracies.append(epoch_accuracy)\n",
        "    val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iLTebhEzQiT2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 5 Define Tunable Model Function**\n",
        "\n",
        "Optuna can be used to optimize hyper parameters as well as model structure and optimizers.  \n",
        "\n",
        "To optimize a components of the model structure such as dropout rate or nunber of layers, we need to createa define_model fuction.\n",
        "\n",
        "In this workshop, we will be optimizing dropout_rate,learning rate, and the optimizer.\n",
        "\n",
        "\n",
        "In order for Optuna to try out different hyperparamter values, you need to suggest a range for it to test using one the **trial.suggest_** methods"
      ],
      "metadata": {
        "id": "aF4zGzn6zDO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define trial hyperpameters like this\n",
        "\n",
        "#hyperparameter = trial.suggest_int(\"hyperparamter\", 1, 5, log=False)\n",
        "\n",
        "#Pass in train.suggest_\"data type\"(hyperparamter name, lower bound, upper bound, log=True or False)"
      ],
      "metadata": {
        "id": "y5Gywz2VL859"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def define_model(trial):\n",
        "  #Optimize the number of layers, and dropout ratio\n",
        "  dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
        "\n",
        "  #Instantiate the model with suggested dropout rate\n",
        "  model = ImageCNN(dropout_rate=dropout_rate)\n",
        "  return model"
      ],
      "metadata": {
        "id": "5aUqYD9ABR4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 6: Define Objective Function**\n",
        "\n",
        "In optuna, your objective function defines the evaluation metric you want to optimise your hyperparameters for.  In our case we will be optimizing the validaiton accuracy.\n",
        "\n",
        "The **Objective Function** is just a regular training loop function wrapped with the optuna optimization so it can run through all **trials**.\n",
        "\n",
        "A **trial** is a single evaluation of the objective function.  During each trial, Optuna generate a unique combination of the hyperparamters you specified.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "31uGGnJ2Jq45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "N_TRAIN_EXAMPLES = batch_size * 30\n",
        "N_VAL_EXAMPLES = batch_size * 10\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "  #Intiantiate a new model for each trail by passing trial to the define_model function\n",
        "  model = define_model(trial).to(device)\n",
        "\n",
        "  #Suggest different optimizers using the trial.suggest_ method\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "  lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "  optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "\n",
        "  #Define the number of epochs for each trial\n",
        "  num_epochs = 20\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "\n",
        "      data, labels = data.to(device), labels.to(device)\n",
        "      if batch_idx * batch_size >= N_TRAIN_EXAMPLES: #Reduce the number of batches for each epoch\n",
        "        break\n",
        "\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    #Validation\n",
        "    model.eval()\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "    #predicted = None\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, labels) in enumerate(val_loader):\n",
        "        model.eval()\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "        if batch_idx * batch_size >= N_VAL_EXAMPLES:\n",
        "          break\n",
        "        output = model(data)\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        running_corrects += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = running_corrects / total_samples\n",
        "\n",
        "    trial.report(accuracy, epoch)\n",
        "\n",
        "    if trial.should_prune():\n",
        "      raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cshevdxez5zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 7 Run the Study**\n",
        "\n",
        "The **study** is the entire hyperparameter optimization process.  It does all of the trials, collects the results, and determines the best hyperparameters for the defined optimization metrics.\n",
        "\n",
        "One of the main advantages of Optuna is **pruning** underperforming trials, drastically reducing the run time."
      ],
      "metadata": {
        "id": "41WkZC2d3-JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Study\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "#Specify the number of trials (n_trials)\n",
        "study.optimize(objective, n_trials=XXXX, timeout=600)\n",
        "\n",
        "#Keep track of pruned and complete trials\n",
        "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
        "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
        "\n",
        "#Print the Statistics\n",
        "print(\"Study statistsics: \")\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "print(\"Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\" Value: \", trial, trial.value)\n",
        "\n",
        "print(\". Params:  \")\n",
        "for key, value in trial.params.items():\n",
        "  print(\"      {}:  {}\".format(key, value))\n"
      ],
      "metadata": {
        "id": "mtdiDZyL4FTQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Challenge Add a Learning Rate Scheduler to be Optimized**\n",
        "\n",
        "Now its time to try to add a new hyperparameter to be tuned!\n",
        "\n",
        "I have defined a StepLR scheduler which takes two hyper parameters **step_size** and **gamma**\n",
        "\n",
        "Define the **step_size** and **gamma** so that they can be tuned by Optuna.\n",
        "\n",
        "**Step_size:** Number of epochs before updating learning rate.\n",
        "\n",
        "**Gamma:** This is the value the learning rate will be mutiplied by at each interval\n",
        "\n"
      ],
      "metadata": {
        "id": "6nqvtNLCQ-v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "N_TRAIN_EXAMPLES = batch_size * 30\n",
        "N_VAL_EXAMPLES = batch_size * 10\n",
        "\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "  #Intiantiate a new model for each trail by passing trial to the define_model function\n",
        "  model = define_model(trial).to(device)\n",
        "\n",
        "  #Suggest different optimizers using the trial.suggest_ method\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
        "  lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
        "  optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "\n",
        "  #Define a learning rate scheduler (StepLR)\n",
        "\n",
        "  step_size = XXXX\n",
        "  gamma = XXXX\n",
        "\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "  #Define the number of epochs for each trial\n",
        "  num_epochs = 20\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "\n",
        "      data, labels = data.to(device), labels.to(device)\n",
        "      if batch_idx * batch_size >= N_TRAIN_EXAMPLES: #Reduce the number of batches for each epoch\n",
        "        break\n",
        "\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    scheduler.step() #step the scheduler\n",
        "\n",
        "\n",
        "    #Validation\n",
        "    model.eval()\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "    #predicted = None\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, labels) in enumerate(val_loader):\n",
        "        model.eval()\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "        if batch_idx * batch_size >= N_VAL_EXAMPLES:\n",
        "          break\n",
        "        output = model(data)\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        running_corrects += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = running_corrects / total_samples\n",
        "\n",
        "    trial.report(accuracy, epoch)\n",
        "\n",
        "    if trial.should_prune():\n",
        "      raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  return accuracy\n",
        "  #Define the number of epochs for each trial\n",
        "  num_epochs = 30\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "\n",
        "      data, labels = data.to(device), labels.to(device)\n",
        "      if batch_idx * batch_size >= N_TRAIN_EXAMPLES: #Reduce the number of batches for each epoch\n",
        "        break\n",
        "\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = model(data)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    #Validation\n",
        "    model.eval()\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "    #predicted = None\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, labels) in enumerate(val_loader):\n",
        "        model.eval()\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "        if batch_idx * batch_size >= N_VAL_EXAMPLES:\n",
        "          break\n",
        "        output = model(data)\n",
        "\n",
        "        _, predicted = torch.max(output, 1)\n",
        "        running_corrects += (predicted == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    accuracy = running_corrects / total_samples\n",
        "\n",
        "    trial.report(accuracy, epoch)\n",
        "\n",
        "    if trial.should_prune():\n",
        "      raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "  return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "lu1SBCbERkBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run the Study Again**"
      ],
      "metadata": {
        "id": "WU8eXV3mY9u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Study\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "#Specify the number of trials (n_trials)\n",
        "study.optimize(objective, n_trials=XXXX, timeout=600)\n",
        "\n",
        "#Keep track of pruned and complete trials\n",
        "pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]\n",
        "complete_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]\n",
        "\n",
        "#Print the Statistics\n",
        "print(\"Study statistsics: \")\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "print(\"Number of pruned trials: \", len(pruned_trials))\n",
        "print(\"Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(\" Value: \", trial, trial.value)\n",
        "\n",
        "print(\". Params:  \")\n",
        "for key, value in trial.params.items():\n",
        "  print(\"      {}:  {}\".format(key, value))"
      ],
      "metadata": {
        "id": "LTAhdWiVZAa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Check out Optuna's Visualizations**"
      ],
      "metadata": {
        "id": "Pm77yZmkcHip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from optuna.visualization import (\n",
        "    plot_optimization_history,\n",
        "    plot_param_importances,\n",
        "    plot_parallel_coordinate,\n",
        "    plot_slice,\n",
        "    plot_contour,\n",
        "    plot_edf,\n",
        "    plot_intermediate_values,\n",
        ")\n",
        "\n",
        "# After running your study:\n",
        "fig1 = plot_optimization_history(study)\n",
        "fig2 = plot_param_importances(study)\n",
        "fig3 = plot_parallel_coordinate(study)\n",
        "fig4 = plot_slice(study)\n",
        "fig5 = plot_contour(study)\n",
        "fig6 = plot_edf(study)\n",
        "fig7 = plot_intermediate_values(study)\n",
        "\n",
        "# Then display the figures\n",
        "fig1.show()\n",
        "fig2.show()\n",
        "fig3.show()\n",
        "fig4.show()\n",
        "fig5.show()\n",
        "fig6.show()\n",
        "fig7.show()"
      ],
      "metadata": {
        "id": "O-LSdd8iabjw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}