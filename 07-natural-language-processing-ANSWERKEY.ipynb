{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl5UV6f9VkIr"
      },
      "source": [
        "# **Natural Language Processing with Pytorch**\n",
        "\n",
        "This notebook will guide you through key **Natural Language Processing (NLP) concepts** using **PyTorch**, from **tokenization** to **deep learning models** like **LSTMs and Transformers**.\n",
        "\n",
        "**Key Topics Covered:**\n",
        "- **Tokenization** (using Hugging Face tokenizers)\n",
        "- **Word Embeddings** (Custom + Pretrained BERT)\n",
        "- **Building an LSTM for NLP**\n",
        "- **Using Transformers for NLP**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Installs, Imports, Seed, and GPU Utilization**"
      ],
      "metadata": {
        "id": "AkS-BDSjVy3K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7xPi4RgVkIv",
        "outputId": "d3c3b454-47c3-4bd3-bb53-b599be58b7d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, get_scheduler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Check if a GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sz6295_VkIw"
      },
      "source": [
        "## **Part 2: Tokenization & Vocabulary Building**\n",
        "\n",
        "Before we can process text using deep learning, we need to **convert text into numerical format**. Tokenization is the process of **splitting text into smaller units** (words, subwords, or characters). We'll use **Hugging Face's AutoTokenizer** for modern subword tokenization instead of traditional methods like NLTK."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgcX0KRxVkIx",
        "outputId": "ab9106d8-f8e2-40f1-f946-644762f62c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized Corpus:\n",
            "[['deep', 'learning', 'is', 'amazing'], ['natural', 'language', 'processing', 'is', 'a', 'branch', 'of', 'ai'], ['p', '##yt', '##or', '##ch', 'makes', 'nl', '##p', 'easier'], ['transformers', 'are', 'powerful', 'models']]\n"
          ]
        }
      ],
      "source": [
        "# Load a pretrained tokenizer (BERT-based)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Sample dataset: A few example sentences\n",
        "corpus = [\n",
        "    \"Deep learning is amazing\",\n",
        "    \"Natural language processing is a branch of AI\",\n",
        "    \"PyTorch makes NLP easier\",\n",
        "    \"Transformers are powerful models\"\n",
        "]\n",
        "\n",
        "# Tokenize the sentences using the BERT tokenizer\n",
        "tokenized_corpus = [tokenizer.tokenize(sentence.lower()) for sentence in corpus]\n",
        "\n",
        "# Let's check how the tokenizer splits words\n",
        "print(\"\\nTokenized Corpus:\")\n",
        "print(tokenized_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGR6boD3VkIx"
      },
      "source": [
        "### Building a Vocabulary\n",
        "Once we have tokenized text, we need to assign **unique numerical indices** to each token to create a vocabulary.\n",
        "We'll create a **word2idx mapping** where each unique token gets a unique number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ydjg1c6VkIy",
        "outputId": "b6ede523-37ac-464e-a268-5e79477a88e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 25\n"
          ]
        }
      ],
      "source": [
        "def build_vocab(corpus):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary dictionary from tokenized text.\n",
        "    Assigns a unique index to each token.\n",
        "    \"\"\"\n",
        "    counter = Counter()\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "\n",
        "    # Create a mapping of words to indices, starting from 2\n",
        "    word2idx = {word: idx+2 for idx, word in enumerate(counter.keys())}\n",
        "\n",
        "    # Add special tokens for unknown words and padding\n",
        "    word2idx[\"<unk>\"] = 0\n",
        "    word2idx[\"<pad>\"] = 1\n",
        "\n",
        "    return word2idx\n",
        "\n",
        "# Build vocabulary from tokenized corpus\n",
        "vocab = build_vocab(tokenized_corpus)\n",
        "print(\"Vocabulary size:\", len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPgjL632VkIy"
      },
      "source": [
        "## **Part 3: Word Embeddings (Custom & Pretrained BERT)**\n",
        "\n",
        "Neural networks **do not understand text** directly, so we need to **convert words into dense numerical vectors**.\n",
        "\n",
        "We'll explore two approaches:\n",
        "- **Custom Embeddings**: Learned during training (random initialization)\n",
        "- **Pretrained Embeddings**: Extracted from BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBJHtUayVkIz",
        "outputId": "cc67c238-6978-4675-9bdd-3adb359fb007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Embedding Shape: torch.Size([1, 4, 50])\n"
          ]
        }
      ],
      "source": [
        "# Custom Word Embeddings (random initialization)\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 50  # number of features per word\n",
        "\n",
        "embedding_layer = nn.Embedding(VOCAB_SIZE, EMBED_DIM).to(device)\n",
        "\n",
        "# Sample sentence to tensor\n",
        "sample_sentence = \"Deep learning is powerful\"\n",
        "sample_tokens = tokenizer.tokenize(sample_sentence.lower())\n",
        "sample_indices = [vocab.get(word, vocab[\"<unk>\"]) for word in sample_tokens]\n",
        "sample_tensor = torch.tensor(sample_indices).unsqueeze(0).to(device)\n",
        "\n",
        "# Get the embedding representation\n",
        "embedded_sentence = embedding_layer(sample_tensor)\n",
        "print(\"\\nCustom Embedding Shape:\", embedded_sentence.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4: Using Pretrained Embeddings (BERT)**\n",
        "\n",
        "Instead of learning word embeddings from scratch, we can use **pretrained embeddings** from models like BERT. These embeddings capture **semantic meaning** and are **contextualized**, meaning they depend on the sentence."
      ],
      "metadata": {
        "id": "GXNh4yPtW2aF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pretrained BERT model\n",
        "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"Deep learning is transforming AI\"\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}  # Move to GPU if available\n",
        "\n",
        "# Get embeddings from BERT\n",
        "with torch.no_grad():\n",
        "    outputs = bert_model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "print(\"BERT Embedding Shape:\", last_hidden_states.shape)  # Shape: (1, seq_length, 768)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TsacRXkW7Y-",
        "outputId": "7a3a505a-8a04-41d6-f1a8-184b2eab9c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT Embedding Shape: torch.Size([1, 7, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”¹ Custom vs Pretrained Embeddings: Key Differences\n",
        "| Feature            | Custom Embeddings | Pretrained (BERT) |\n",
        "|--------------------|------------------|------------------|\n",
        "| Trained on dataset | Yes              | No (already trained) |\n",
        "| Contextualized?    | No               | Yes |\n",
        "| Captures semantics | Limited          | Strong |\n",
        "| Computational cost | Low              | High |"
      ],
      "metadata": {
        "id": "U-PXEmr4W9JJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 5: Applying Transformers for NLP**\n",
        "\n",
        "### **Understanding the IMDB Dataset and the Task**\n",
        "\n",
        "The IMDB dataset is a widely used benchmark dataset for sentiment analysis. It consists of 50,000 movie reviews from IMDB, labeled as either positive (1) or negative (0). The dataset is divided into:\n",
        "* 25,000 training reviews\n",
        "* 25,000 test reviews\n",
        "\n",
        "Each set contains an equal number of positive and negative reviews. Here are two examples:\n",
        "* âœ… Positive Review: \"This movie was absolutely fantastic! The storyline was engaging, and the performances were top-notch.\"\n",
        "* âŒ Negative Review: \"Terrible film. The acting was bad, the plot was predictable, and I regret watching it.\"\n",
        "\n",
        "The goal is to build a binary classification model that takes a movie review as input and predicts whether it is positive (1) or negative (0).\n",
        "\n",
        "**Why is this useful?**\n",
        "\n",
        "* Companies use sentiment analysis to analyze customer feedback.\n",
        "* It helps in opinion mining from social media, news, and reviews.\n",
        "* Used in automated content moderation (e.g., filtering harmful content).\n",
        "\n",
        "**Challenges in Text-Based Sentiment Analysis**\n",
        "\n",
        "Unlike structured numerical data, text data presents unique challenges:\n",
        "* Different sentence structures:\n",
        "  * \"I didn't like this movie.\" (Negative)\n",
        "  * \"I didn't expect to like this movie, but I did.\" (Positive)\n",
        "* Sarcasm & Negation Handling:\n",
        "  * \"Oh great, another terrible remake.\" (Negative)\n",
        "* Word Importance:\n",
        "  * Some words (e.g., \"not\") can flip the sentiment of a sentence.\n",
        "* Large Vocabulary:\n",
        "  * Many unique words â†’ Requires efficient embedding techniques.\n",
        "\n",
        "**We Will Solve This Using BERT**\n",
        "\n",
        "BERT (Bidirectional Encoder Representations from Transformers) is a pretrained deep learning model that:\n",
        "* Understands context in text (unlike older methods like TF-IDF)\n",
        "* Is pretrained on vast amounts of text data\n",
        "* Handles long-range dependencies in text\n",
        "\n",
        "We will fine-tune BERT to classify IMDB reviews as positive or negative using PyTorch and Hugging Faceâ€™s transformers library."
      ],
      "metadata": {
        "id": "sB9jhk88XbuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 1: Load the Data"
      ],
      "metadata": {
        "id": "vLVu-s5yiRem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset (already pre-split into train and test sets)\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Reduce dataset size to save memory (use only 500 training samples)\n",
        "train_texts = dataset['train']['text'][:500]\n",
        "train_labels = dataset['train']['label'][:500]\n",
        "\n",
        "# Use full test set for evaluation\n",
        "test_texts = dataset['test']['text'][:100]\n",
        "test_labels = dataset['test']['label'][:100]\n",
        "\n",
        "# Print dataset size\n",
        "print(f\"Training samples: {len(train_texts)}\")\n",
        "print(f\"Testing samples: {len(test_texts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCS2Qgh2iaMU",
        "outputId": "12347377-3af2-4880-fb69-e11efa8d0a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 500\n",
            "Testing samples: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 2: Tokenize the Data"
      ],
      "metadata": {
        "id": "yC7BJDYeibBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer for DistilBERT (a smaller version of BERT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Function to tokenize text\n",
        "def tokenize_function(texts):\n",
        "    return tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "\n",
        "# Tokenize training and testing texts\n",
        "train_encodings = tokenize_function(train_texts)\n",
        "test_encodings = tokenize_function(test_texts)\n",
        "\n",
        "# Convert labels into PyTorch tensors\n",
        "train_labels_tensor = torch.tensor(train_labels)\n",
        "test_labels_tensor = torch.tensor(test_labels)"
      ],
      "metadata": {
        "id": "gUwBlrYHifyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 3: Create DataSet Class"
      ],
      "metadata": {
        "id": "YJ3rB7vsigNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class to handle tokenized data\n",
        "class IMDBDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings  # Tokenized text\n",
        "        self.labels = labels  # Corresponding labels (0 = negative, 1 = positive)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)  # Number of samples in dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve tokenized text and label for a given index\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# Create dataset objects for training and testing\n",
        "train_dataset = IMDBDataset(train_encodings, train_labels_tensor)\n",
        "test_dataset = IMDBDataset(test_encodings, test_labels_tensor)"
      ],
      "metadata": {
        "id": "7PPHi1MXimT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 4: Create DataLoaders"
      ],
      "metadata": {
        "id": "5q345cbPimwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "03sAK0x4iq_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 5: Load Pretrained DistilBERT Model"
      ],
      "metadata": {
        "id": "CKpnHYxkiwJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained DistilBERT model for binary classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Move model to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaEkUEvJi3LK",
        "outputId": "35492663-348e-4af8-b05b-22c70b048370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x TransformerBlock(\n",
              "          (attention): DistilBertSdpaAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 7: Define Optimizer & Loss Function"
      ],
      "metadata": {
        "id": "HhYqm8gTi6GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Calculate the number of training steps (total batches in 2 epochs)\n",
        "num_training_steps = len(train_loader) * 2\n",
        "\n",
        "# Create a learning rate scheduler (linearly decreases learning rate)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")"
      ],
      "metadata": {
        "id": "5tgcHdI9jA_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 8: Training Loop"
      ],
      "metadata": {
        "id": "yHU9KOmgjDw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time  # Add timing to monitor training speed\n",
        "\n",
        "# Enable mixed precision training to reduce memory usage\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "num_epochs = 1  # Reduce to 1 epoch for faster training\n",
        "start_time = time.time()  # Start timer\n",
        "\n",
        "model.train()  # Set model to training mode\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Starting epoch {epoch+1}...\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        batch = {key: val.to(device) for key, val in batch.items()}  # Move to device\n",
        "\n",
        "        with torch.cuda.amp.autocast():  # Mixed precision\n",
        "            outputs = model(**batch)  # Forward pass\n",
        "            loss = outputs.loss  # Compute loss\n",
        "\n",
        "        # Backpropagation with mixed precision scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Print loss every 10 batches\n",
        "        if batch_idx % 10 == 0:\n",
        "          print(f\"Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # If training takes too long (> 10 minutes), break\n",
        "        if time.time() - start_time > 600:\n",
        "            print(\"Training took too long, stopping early.\")\n",
        "            break\n",
        "\n",
        "    print(f\"Epoch {epoch+1} completed. Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz2i0whnjHfh",
        "outputId": "5d618ccc-8697-4cd3-81ed-9d8919d0a6ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-bb507ffa0945>:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "<ipython-input-12-bb507ffa0945>:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():  # Mixed precision\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1...\n",
            "Batch 0/63 - Loss: 0.7323\n",
            "Batch 10/63 - Loss: 0.0293\n",
            "Batch 20/63 - Loss: 0.0067\n",
            "Batch 30/63 - Loss: 0.0030\n",
            "Batch 40/63 - Loss: 0.0021\n",
            "Batch 50/63 - Loss: 0.0013\n",
            "Training took too long, stopping early.\n",
            "Epoch 1 completed. Loss: 0.0013\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### STEP 9: Evaluation"
      ],
      "metadata": {
        "id": "bg6mutR9iK0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model to evaluation mode (disables dropout and gradients)\n",
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "true_labels = []\n",
        "\n",
        "# Disable gradient computation to save memory during testing\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {key: val.to(device) for key, val in batch.items()}  # Move batch to device\n",
        "        outputs = model(**batch)  # Forward pass\n",
        "        logits = outputs.logits  # Get model outputs\n",
        "\n",
        "        # Convert logits to class predictions (0 or 1)\n",
        "        preds = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "        # Store predictions and actual labels for accuracy calculation\n",
        "        predictions.extend(preds)\n",
        "        true_labels.extend(labels)\n",
        "\n",
        "# Compute accuracy score\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yo6uAcBViHaH",
        "outputId": "c221c0e9-d557-4922-ff21-0f2670c27041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 7: Challenge Time!**\n",
        "\n",
        "**Try modifying the training setup!**\n",
        "- Change the learning rate and optimizer\n",
        "- Use a different model (e.g., 'distilbert-base-uncased')\n",
        "- Train for more epochs\n",
        "- Add regularization like dropout\n",
        "- Experiment with different batch sizes\n",
        "- Visualize loss over training steps"
      ],
      "metadata": {
        "id": "3eoMGWw9bWR0"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}